import re

from consts import TOKEN, PUNCTUATIONS, HTML_ESCAPE_DICT


class WordTokenizer:
    def __init__(self, regex=None, keep_words=()):
        # è§„åˆ™åˆ‡è¯ï¼Œå¯ä»¥ä¿ç•™å“ªäº›è¯ä¸ç”¨åˆ‡åˆ†

        html_escape = list(HTML_ESCAPE_DICT.keys())
        # åŒä¸€ä¸ªå­—ç¬¦å¼€å¤´æ»¡è¶³å¤šä¸ªè§„åˆ™ï¼Œä»¥ç¬¬ä¸€ä¸ªè§„åˆ™ä½œæ•°
        if not regex:
            # æ•°å­—ï¼ˆæ•´æ•°å’Œå°æ•°ï¼‰| è‹±æ–‡å•è¯ | ç©ºç™½ç¬¦ | ä¸­æ–‡ | å¸Œè…Šå¤§å†™å­—æ¯ | å¸Œè…Šå°å†™å­—æ¯ | æ ‡ç‚¹ | å…¶ä»–å­—ç¬¦
            regex = f"\d+\.?\d+|[A-Za-z]+|\s+|[\u4e00-\u9fa5]|[\u0391-\u03a9]|[\u03b1-\u03c9]|[{PUNCTUATIONS}]|[^a-zA-Z0-9{PUNCTUATIONS}\s\u4e00-\u9fa5\u03b1-\u03c9\u0391-\u03a9]"
            regex = '|'.join(html_escape) + '|' + regex
        if keep_words:
            regex = '|'.join(keep_words) + '|' + regex
        self.find_re = re.compile(regex)

    def run(self, text):
        # è¾“å…¥æ–‡æœ¬ï¼Œè¾“å‡ºåˆ‡åˆ†åçš„æ–‡æœ¬ä»¥åŠåæ ‡--start-end--- æœ€åä¸€ä¸ªå­—ç¬¦çš„åæ ‡ï¼Œè€Œä¸æ˜¯ç´¢å¼•
        res = []
        tmp_index = 0

        token_index = 0
        for i in self.find_re.finditer(text):
            if tmp_index < i.start():
                if text[tmp_index:i.start()].strip():
                    res.append(TOKEN(
                        index=token_index,
                        start_offset=tmp_index,
                        end_offset=i.start() - 1,
                        text=text[tmp_index:i.start()],
                        label='O'))
                    token_index+=1
            if i.group().strip():
                res.append(TOKEN(token_index,i.start(), i.end() - 1, i.group(), 'O'))
                token_index += 1
            tmp_index = i.end()

        if (tmp_index < len(text)) and (text[tmp_index:len(text) - 1].strip()):

            res.append(TOKEN(token_index,tmp_index, len(text) - 1, text[tmp_index:len(text) - 1], 'O'))
            token_index += 1
        return res  # [Token(start,end,text)]

    # def _cut_sentence_to_words_zh(self, sentence: str):
#         english = 'abcdefghijklmnopqrstuvwxyz0123456789Î±Î³Î²Î´ÎµÎ¶Î·Î¸Î¹ÎºÎ»Î¼Î½Î¾Î¿Ï€ÏÏƒÏ„Ï…Ï†Ï‡ÏˆÏ‰'
#         output = []
#         buffer = ''
#         for s in sentence:
#             if s in english or s in english.upper():  # è‹±æ–‡æˆ–æ•°å­—
#                 buffer += s
#             else:  # ä¸­æ–‡
#                 if buffer:
#                     output.append(buffer)
#                 buffer = ''
#                 output.append(s)
#         if buffer:
#             output.append(buffer)
#         return output

    # def get_word_tokenizer(self, method='nltk_word_tokenizer'):
    #     if method == 'nltk_word_tokenizer':
    #         word_tokenizer = nltk.NLTKWordTokenizer().tokenize
    #     elif method == 'nltk_word_tokenizer_span':
    #         word_tokenizer = nltk.WordPunctTokenizer().tokenize
    #     elif method == 'm2':
    #         word_tokenizer = ''
    #         pass
    #         # from transformers import BasicTokenizer
    #         # BasicTokenizer(do_lower_case=False).tokenize()
    #     else:
    #         word_tokenizer = re.compile(r'[ï¼Œ,:ï¼š;ï¼›\s]').split
    #     return word_tokenizer


class SentenceTokenizer:
    def __init__(self, keep_suffix=()):
        from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters
        punkt_param = PunktParameters()

        abbreviation = ['et al.', 'i.e.', 'e.g.', 'etc.', 'i.e', 'e.g', 'etc', ' et al']
        if keep_suffix:
            abbreviation.extend(list(keep_suffix))
        punkt_param.abbrev_types = set(abbreviation)
        self.sentence_tokenizer = PunktSentenceTokenizer(punkt_param, verbose=True).tokenize

    def run(self, text):
        tokens = self.sentence_tokenizer(text)
        end = 0
        token_index = 0
        all_token_span = []
        for each_token in tokens:
            all_token_span.append(TOKEN(
                token_index,
                text[end:].find(each_token) + end,
                text[end:].find(each_token) + end + len(each_token) - 1,  # åæ ‡è€Œä¸æ˜¯ç´¢å¼•
                each_token,
                'O'
            ))
            token_index+=1
            end += len(each_token)
        return all_token_span  # sentence_token , start ,end


#     def _cut_paragraph_to_sentences_zh(self, para: str, drop_empty_line=True, strip=True, deduplicate=False):
#         """
#         Args:
#            para: è¾“å…¥æ–‡æœ¬
#            drop_empty_line: æ˜¯å¦ä¸¢å¼ƒç©ºè¡Œ
#            strip:  æ˜¯å¦å¯¹æ¯ä¸€å¥è¯åšä¸€æ¬¡strip
#            deduplicate: æ˜¯å¦å¯¹è¿ç»­æ ‡ç‚¹å»é‡ï¼Œå¸®åŠ©å¯¹è¿ç»­æ ‡ç‚¹ç»“å°¾çš„å¥å­åˆ†å¥
#
#         Returns:
#            sentences: list of str
#         """
#         if deduplicate:
#             para = re.sub(r"([ã€‚ï¼ï¼Ÿ\!\?])\1+", r"\1", para)
#
#         para = re.sub('([ã€‚ï¼ï¼Ÿ\?!])([^â€â€™])', r"\1\n\2", para)  # å•å­—ç¬¦æ–­å¥ç¬¦
#         para = re.sub('(\.{6})([^â€â€™])', r"\1\n\2", para)  # è‹±æ–‡çœç•¥å·
#         para = re.sub('(\â€¦{2})([^â€â€™])', r"\1\n\2", para)  # ä¸­æ–‡çœç•¥å·
#         para = re.sub('([ã€‚ï¼ï¼Ÿ\?!][â€â€™])([^ï¼Œã€‚ï¼ï¼Ÿ\?])', r'\1\n\2', para)
#         # å¦‚æœåŒå¼•å·å‰æœ‰ç»ˆæ­¢ç¬¦ï¼Œé‚£ä¹ˆåŒå¼•å·æ‰æ˜¯å¥å­çš„ç»ˆç‚¹ï¼ŒæŠŠåˆ†å¥ç¬¦\næ”¾åˆ°åŒå¼•å·åï¼Œæ³¨æ„å‰é¢çš„å‡ å¥éƒ½å°å¿ƒä¿ç•™äº†åŒå¼•å·
#         para = para.rstrip()  # æ®µå°¾å¦‚æœæœ‰å¤šä½™çš„\nå°±å»æ‰å®ƒ
#         # å¾ˆå¤šè§„åˆ™ä¸­ä¼šè€ƒè™‘åˆ†å·;ï¼Œä½†æ˜¯è¿™é‡Œæˆ‘æŠŠå®ƒå¿½ç•¥ä¸è®¡ï¼Œç ´æŠ˜å·ã€è‹±æ–‡åŒå¼•å·ç­‰åŒæ ·å¿½ç•¥ï¼Œéœ€è¦çš„å†åšäº›ç®€å•è°ƒæ•´å³å¯ã€‚
#         sentences = para.split("\n")
#         if strip:
#             sentences = [sent.strip() for sent in sentences]
#         if drop_empty_line:
#             sentences = [sent for sent in sentences if len(sent.strip()) > 0]
#         return sentences


if __name__ == '__main__':
    s4 = "CX (cisplatin 80 mg/m(2) IV Q3W; capecitabine 1000 mg/m(2) P.O. BID for 14 days Q3W) plus intravenous AMG 386 10 mg/kg QW (Arm A"
    t = SentenceTokenizer(keep_suffix=('s.v',))

    print(t.run(s4))

    s = "Oral acalabruti(nib) 100â€‰mg æˆ‘iåœ¨å‘å•Š <  twic\u00b7e daily  ; was & admi\nnister\ted with or &lt; 13.8kg/m without,Î±-1 12.ğŸ˜Š "
    w_t = WordTokenizer(regex=None, keep_words=('åœ¨å‘',))
    print(w_t.run(s))
