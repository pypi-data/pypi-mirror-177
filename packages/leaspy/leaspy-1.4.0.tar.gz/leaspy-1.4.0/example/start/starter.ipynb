{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not recommand to run the notebook from within the Leaspy repo (to prevent it from interfering with Leaspy git versioning).\n",
    "Therefore, copy paste the start folder elsewhere, and replace the `project_path` with the path to the leaspy folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these 2 lines are not needed if you installed leaspy with pip\n",
    "leaspy_path = os.path.join(os.getcwd(), '..', '..')\n",
    "sys.path.append(leaspy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from leaspy import Leaspy, Data, AlgorithmSettings, IndividualParameters, __watermark__\n",
    "# Watermark trace with all packages versions\n",
    "__watermark__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Data\n",
    "\n",
    "You have to store your data into a `Data` object.\n",
    "\n",
    "### Option #1\n",
    "You can load your data directly from a csv file, whose rows corresponds to subject visits, each column being a feature - except for the `ID` and `TIME` column. This result in multiple rows per subject. The input format has therefore to follow the following rules:\n",
    "- A column named `ID`: corresponds to the subject indices\n",
    "- A columns named `TIME`: corresponds to the subject's age at the corresponding visit\n",
    "- One column per feature\n",
    "- Each row is a visit, therefore the concatenation of the subject ID, the patient age at which the corresponding visit occured, and then the feature values.\n",
    "\n",
    "Here is an example :\n",
    "\n",
    "| ID | TIME | Feature 1 | Feature 2 | Feature 3 | Feature 4\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| 309 | 70.4 | 25 | 0.12 | -0.19 | 3 |\n",
    "| 309 | 73.4 | 10 | 0.13 | NaN | 1.3 |\n",
    "| 309 | 75.7 | 24.3 | 0.0 | -0.44 | 0.8 |\n",
    "| 40 | 60.1 | 5 | NaN | -0.12 | 2.3 |\n",
    "| 40 | 62.5 | 23.4 | 0.8 | -0.142 | 0.94 |\n",
    "| 918 | 88 | 9.3 | 0.9 | -0.3 | 0.23 |\n",
    "| 918 | 92.3 | 33 | 0.88 | -0.423 | NaN |\n",
    "| 918 | 95.3 | 34 | 0.58 | -0.523 | 0.34 |\n",
    "\n",
    "You can find additional example in the `inputs` folder: `data.csv` and `data_univariate.csv`\n",
    "\n",
    "You probably noticed that there are NaN: do not worry, Leaspy can handle them ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data.from_csv_file(os.path.join('inputs', 'data_normalized.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option #2\n",
    "\n",
    "However, there are chances that your csv files contains data that either you don't want or that don't fit the model you are going to use. For instance, to build a logistic curve model, you need data that are normalized between 0 and 1 and that are increasing with time. Moreover, to calibrate the progression model, **we highly recommend to keep subjects that have been seen at least two times**. \n",
    "\n",
    "For that reason, you can load your data from a dataframe that you have preprocessed with pandas methods.\n",
    "\n",
    "Let's seen an example where:\n",
    "- Feature 1 is increasing through time and is between 0 and 1: nothing to do\n",
    "- Feature 2 is increasing through time and is between 0 and 100: need to normalize between 0 and 1\n",
    "- Feature 3 is decreasing through time and is between 0 and 1: need to revert the temporal progression to be increasing\n",
    "- Feature 4 is decreasing through time and is between 60 and 0: need to revert and to normalize between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join('inputs', 'data.csv'))\n",
    "\n",
    "# —— Feature 1\n",
    "df['Feature 1'] = df['Feature 1']\n",
    "\n",
    "# —— Feature 2\n",
    "df['Feature 2'] = df['Feature 2']/100\n",
    "\n",
    "# —— Feature 3\n",
    "df['Feature 3'] = -df['Feature 3']+1\n",
    "\n",
    "# —— Feature 4\n",
    "df['Feature 4'] = -df['Feature 4']/60+1\n",
    "\n",
    "# —— Let's select patient with at least two visits\n",
    "df = df.set_index(['ID', 'TIME'])\n",
    "indices = [idx for idx in df.index.unique('ID') if df.loc[idx].shape[0] >= 2]\n",
    "df = df[df.index.get_level_values(0).isin(indices)]\n",
    "\n",
    "# —— Store the data into a Data object\n",
    "data = Data.from_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark & Tip:** while some data have natural minimum and maximum values (inducing ceiling and floor effects), other don't have natural bounds. In that case, normalize as best as you can so that you don't have values (current or future) higher than 1 or lower than 0. You can check that with `Feature 4` that does not reach maximum values or `Feature 1` that starts above minimum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fts_min_max = df.agg(['min','max'])\n",
    "\n",
    "assert (fts_min_max.loc['min'] >= 0).all()\n",
    "assert (fts_min_max.loc['max'] <= 1).all()\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "fts_min_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Settings\n",
    "\n",
    "To run any algorithm, you need to specify the settings of the related algorithm thanks to the `AlgorithmSettings` object. \n",
    "\n",
    "### Option #1\n",
    "\n",
    "To ease Leaspy's usage for new users, we specified default values for each algorithm. Therefore, the name of the algorithm used is enough to run it. The one you need to fit your progression model is `mcmc_saem`. Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_settings = AlgorithmSettings('mcmc_saem', \n",
    "                                  n_iter=3000,           # n_iter defines the number of iterations\n",
    "                                  progress_bar=True)     # To display a nice progression bar during calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to store the different logs of the model during the iterations, you can use the following method with the path of the folder where the logs will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_settings.set_logs(\n",
    "    path='outputs/logs', # Creates a logs file ; if existing, ask if rewrite it\n",
    "    save_periodicity=50, # Saves the values in csv files every N iterations\n",
    "    console_print_periodicity=100, # Displays logs in the console/terminal every N iterations, or None\n",
    "    plot_periodicity=1000, # Generates the convergence plots every N iterations\n",
    "    overwrite_logs_folder=True # if True and the logs folder already exists, it entirely overwrites it\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Fit model\n",
    "\n",
    "Now that you have your data and your settings, you can run the model. You first need to choose the type of progression shape you want to give to your data. You can either choose logistic or linear (soon exponential) with the possibility to enforce a \"parallelism\" between the features. The dedicated names are  :\n",
    "\n",
    "`logistic`, `logistic_parallel`, `linear` and `linear_parallel`. You can also call the `univariate` model which is a single logistic function that you can run on the `input/data_univariate.csv`.\n",
    "\n",
    "Note : the model might rely on hyperparameters that you can define as shown below. There are optional. The most important one is the `source_dimension`. To be short, if you consider `N` features, these can be reordered as not everyone follows the same relative ordering of events. However, all reordering are not likely to exist, and there are some common patterns within a population. Therefore, the `source_dimension` is in a way a subspace that defines the degrees of freedom that you have to reorder you sequence of events. Still not clear? That's fine. It's related to the `space-shifts` in papers described in the repo. But don't worry if you didn't get it yet. Make it equal to somewhat higher than the log/square-root of the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaspy = Leaspy(\"logistic\", \n",
    "                source_dimension=2, # Optional\n",
    "                noise_model='gaussian_diagonal', # Optional: To get a noise estimate per feature keep it this way (default)\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You are now ready to run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "leaspy.fit(data, settings=algo_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Did it run well?\n",
    "\n",
    "Before assuming that the model is estimated, you have to check that the convergence went went. For that, you can look  the at the convergence during the iterations. To do so, you can explore the `logs` folder that shows the model convergence during the iterations. The first thing to look at is probably the `plots/convergence_1.pdf` and `plots/convergence_2.pdf` files : a run has had enough iterations to converge if the last 20 or even 30% of the iterations were stable for all the parameters. If not, you should provably rerun it with more iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('./outputs/logs/plots/convergence_1.pdf', width=990, height=670)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can check out the parameters of the model that are stored here : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaspy.model.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are probably not straightfoward for now. The most important one is probably `noise_std`. It corresponds to the standard deviation of the error for a given feature. The smallest, the better - up to the lower bound which is the intrinsic noise in the data. Let's display it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = leaspy.model.parameters['noise_std']\n",
    "features = leaspy.model.features\n",
    "\n",
    "print('Standard deviation of the residual noise for feature:')\n",
    "for n, f in zip(noise, features):\n",
    "    print(f'- {f}: {n:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** Usually, cognitive measurements have an intrinsic error (computed on test-retest exams) between 5 and 10%.\n",
    "\n",
    "### Save and load the model\n",
    "\n",
    "There are many reasons why one might want to save the output parameters of the model. To do so, just do: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaspy.save(\"outputs/model_parameters.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, you can load the model if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaspy = Leaspy.load('outputs/model_parameters.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the average model\n",
    "\n",
    "Now that we have sufficient evidence that the model has converged, let's output what the average progression look like! \n",
    "\n",
    "First, let's detail a bit what we are going to represent. We are going to display a trajectory: it corresponds to the temporal progression of the biomarkers. There is not only one trajectory for a cohort, as each subject has his or her own specific trajectory, meaning his or her disease progression. Each of these individual trajectories are rely on individual parameters that are subject-specific. We will see what this individual parameters a bit later, do not worry. For now, let's stick to the \"average\" trajectory.\n",
    "\n",
    "So what does the average trajectory corresponds to? The average trajectory corresponds to a \"virtual\" patient whose individual parameters are the average individual parameters. And these averages are already estimated during the calibration.\n",
    "\n",
    "So let's get these average individual parameters and store them in the `IndividualParameters` object that stores a collection of individual parameter, for many subjects. Here, it is used for only one subject, called `average`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# —— Get the average individual parameters\n",
    "mean_xi = leaspy.model.parameters['xi_mean'].numpy()\n",
    "mean_tau = leaspy.model.parameters['tau_mean'].numpy()\n",
    "mean_source = leaspy.model.parameters['sources_mean'].numpy().tolist()\n",
    "number_of_sources = leaspy.model.source_dimension\n",
    "mean_sources = [mean_source]*number_of_sources\n",
    "\n",
    "# —— Store the average individual parameters in a dedicated object\n",
    "average_parameters = {\n",
    "    'xi': mean_xi,\n",
    "    'tau': mean_tau,\n",
    "    'sources': mean_sources\n",
    "}\n",
    "\n",
    "ip_average = IndividualParameters()\n",
    "ip_average.add_individual_parameters('average', average_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second very important function - after `leaspy.fit()` - is `leaspy.estimate`. Given some individual parameters and timepoints, the function estimates the values of the biomarkers at the given timepoints which derive from the individual trajectory encoded thanks to the individual parameters.\n",
    "\n",
    "You can check out the documentation and run it : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?leaspy.estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timepoints = np.linspace(55, 105, 100)\n",
    "values = leaspy.estimate({'average': timepoints}, ip_average)\n",
    "\n",
    "def plot_trajectory(timepoints, reconstruction, observations=None, *, \n",
    "                    xlabel='Reparametrized age', ylabel='Normalized feature value'):\n",
    "\n",
    "    if observations is not None:\n",
    "        ages = observations.index.values\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.ylim(0, 1)\n",
    "    colors = ['#003f5c', '#7a5195', '#ef5675', '#ffa600']\n",
    "    \n",
    "    for c, name, val in zip(colors, leaspy.model.features, reconstruction.T):\n",
    "        plt.plot(timepoints, val, label=name, c=c, linewidth=3)\n",
    "        if observations is not None:\n",
    "            plt.plot(ages, observations[name], c=c, marker='o', markersize=12, \n",
    "                     linewidth=1, linestyle=':')\n",
    "    \n",
    "    plt.xlim(min(timepoints), max(timepoints))\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_trajectory(timepoints, values['average'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Personalize the model to individual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The personalization procedure allows to estimate the individual parameters that allows to modify the average progression to individual observations. The variations from the average trajectory to the individual one are encoded within three individual parameters : \n",
    "- $\\alpha_i = \\exp(\\xi_i)$ : the acceleration factor, that modulates the speed of progression : $\\alpha_i > 1$ means faster, $\\alpha_i < 1$ means slower than the average progression\n",
    "- $\\tau_i$ : the time shift which delays the progression in a given number of years. It has to be compared to  `tau_mean` $ = \\bar{\\tau} $  which is in the model parameters above. In fact, $ \\tau_i \\sim N( \\bar{\\tau}, \\sigma_{\\tau}^2)$ , so $\\tau_i > \\bar{\\tau}$ means that the patient has a disease that starts later than average, while $\\tau_i < \\bar{\\tau}$ means that the patient has a disease that starts earlier than average\n",
    "- $w_i = (w_1, ..., w_N)$ ($N$ being the size of the feature space) : the space-shift  which might, for a given individual, change the ordering of the conversion of the different features, compared to the mean trajectory.\n",
    "\n",
    "In a nutshell, the $k$-th feature at the $j$-th visit of subject $i$, which occurs at time $t_{ij}$ writes: \n",
    "\n",
    "$$y_{ijk} = f_\\theta ( w_{ik}+ \\exp(\\xi_i) * (t_{ij} - \\tau_i) ) + \\epsilon_{ijk}$$\n",
    "\n",
    "This writing is not exactly correct but helps understand the role of each individual parameters.\n",
    "\n",
    "**[ Advanced ]** Remember the `sources`, or the `source_dimension`? Well, $w_i$ is not estimated directly, but rather thanks to a Independant Component Analysis, such that $w_i = A s_i$ where $s_i$ is of size $N_s$ = `source_dimension`. See associated papers for further details.\n",
    "\n",
    "Now, let's estimate these individual parameters. The procedure relies on the `scipy_minimize` algorithm that you have to define (or to load from an appropriate json file) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1\n",
    "settings_personalization = AlgorithmSettings('scipy_minimize', use_jacobian=True)\n",
    "\n",
    "# Option 2\n",
    "settings_personalization = AlgorithmSettings.load('inputs/algorithm_settings_personalization.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the third most important function of leaspy : `leaspy.personalize`. It estimates the individual parameters for the data you provide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?leaspy.personalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = leaspy.personalize(data, settings_personalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that you can personalize your model on patients that have only one visit! You don't have to use the same `data` as previously. Especially, you can here personalize your model with patients that have only one visit.\n",
    "\n",
    "Once the personalization is done, you can check the different functions that the `IndividualParameters` provides (you can load and save them, transform them to dataframes, etc) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?ip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see what you can do with the individual parameters.\n",
    "\n",
    "# Step 5. Impute missing values & predict individual trajectories\n",
    "\n",
    "The individual parameters entirely defines the individual trajectory, and thus, the biomarker values at any time. So you can reconstruct the individual biomarkers at different ages. \n",
    "\n",
    "You can reconstruct your observations at seen ages, i.e. at visits that have been used to personalize the model. There are two reasons you might want to do that:\n",
    "- see how well the model fitted individual data\n",
    "- impute missing values: as Leaspy handles missing values, it can then reconstruct them (note that this reconstruction will be noiseless)\n",
    "\n",
    "To do so, let's first retrieve the observations of subject '1634-S2-1' in the initial dataset. You can also get his/her individual parameters as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = '1634-S2-1'\n",
    "observations = df.loc[subject_id]\n",
    "print(f'Seen ages: {observations.index.values}')\n",
    "print(ip[subject_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as done with the average trajectory, let's estimate the trajectory for this patient.\n",
    "\n",
    "**Remark**: The `estimate` first argument is a dictionary, so that you can estimate the trajectory of multiple individuals simultaneously (as long as the individual parameters of all your queried patients are in `ip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timepoints = np.linspace(77, 85, 100)\n",
    "reconstruction = leaspy.estimate({subject_id: timepoints}, ip)\n",
    "\n",
    "plot_trajectory(timepoints, reconstruction[subject_id], observations, xlabel='Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot, we clearly see that the estimations at late ages, above 82 years old are clearly prediction. To estimate the quality of the prediction, you can hide some future visits in the calibration and personalization procedure, then estimate the values at these visits and see how good it performs by comparing the predicting value compared to the hidden one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. Further Analysis\n",
    "\n",
    "Besides prediction, the individual parameters are interesting in the sense that they provide meaningful and interesting insights about the disease progression. For that reasons, these individual parameters can be correlated to other cofactors. Let's seen an example.\n",
    "\n",
    "Let's consider that you have a covariate `Cofactor 1` that encodes a genetic status: 1 if a specific mutation is present, 0 otherwise. Now, let's see if this mutation has an effect on the disease progression: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# —— Convert individual parameters to dataframe\n",
    "df_ip = ip.to_dataframe()\n",
    "\n",
    "# —— Merge with cofactors\n",
    "cofactor = pd.read_csv('inputs/cofactor.csv', index_col=['ID'])\n",
    "df_ip = df_ip.join(cofactor)\n",
    "\n",
    "# —— Separate the individual parameters with respect to the cofactor\n",
    "carriers = df_ip[df_ip['Cofactor 1'] == 0.]\n",
    "non_carriers = df_ip[df_ip['Cofactor 1'] == 1.]\n",
    "\n",
    "def plot_histo(title, ft, bins_nb=10):\n",
    "    \n",
    "    # compute bins (same for 2 carriers & non-carriers)\n",
    "    _, bins = np.histogram(df_ip[ft], bins=bins_nb)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.hist(carriers[ft], density=True, bins=bins, edgecolor='white', label='Carriers')\n",
    "    plt.hist(non_carriers[ft], density=True, bins=bins, alpha=0.6, edgecolor='white', label='Non carriers')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# —— Plot the time shifts in carriers and non-carriers\n",
    "plot_histo('Time shift histogram', 'tau')\n",
    "\n",
    "# —— Plot the acceleration factor in carriers and non-carriers\n",
    "plot_histo('Log-Acceleration factor histogram', 'xi')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the mutation has an effect on the disease onset, but not on its pace. Let's verify it with a statistical test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# —— Student t-test (under the asumption of a gaussian distribution only)\n",
    "print(stats.ttest_ind(carriers['tau'], non_carriers['tau']))\n",
    "print(stats.ttest_ind(carriers['xi'], non_carriers['xi']))\n",
    "\n",
    "# —— Mann-withney t-test\n",
    "print(stats.mannwhitneyu(carriers['tau'], non_carriers['tau']))\n",
    "print(stats.mannwhitneyu(carriers['xi'], non_carriers['xi']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now clear that the mutation has an effect on the disease onset, but not on its pace.\n",
    "\n",
    "\n",
    "\n",
    "# The end of this Notebook. But the beginning of your explorations.\n",
    "\n",
    "You've now seen most of the applications that you can run with Leaspy. There are many more to discover based on your experimental settings and analysis. But it's time for you to try it on your own and to enjoy the package.\n",
    "\n",
    "Have fun! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
